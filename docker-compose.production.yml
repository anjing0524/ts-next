# ================================
# OAuth 2.1 System - Production Docker Compose
# ================================
# 生产级 Docker Compose 配置
# 使用方法: docker-compose -f docker-compose.production.yml up -d

version: '3.8'

# ================================
# 网络配置
# ================================
networks:
  oauth-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

# ================================
# 卷配置 (持久化存储)
# ================================
volumes:
  oauth-db-data:
    driver: local
  oauth-logs:
    driver: local
  pingora-logs:
    driver: local
  admin-portal-data:
    driver: local
  letsencrypt-certs:
    driver: local

# ================================
# 服务定义
# ================================
services:

  # ============================
  # Pingora Proxy (反向代理)
  # ============================
  pingora-proxy:
    build:
      context: ./apps/pingora-proxy
      dockerfile: Dockerfile
    container_name: oauth-pingora-proxy
    restart: unless-stopped
    ports:
      - "80:80"      # HTTP
      - "443:443"    # HTTPS
    networks:
      oauth-network:
        ipv4_address: 172.20.0.10
    volumes:
      - ./apps/pingora-proxy/config:/app/config:ro
      - pingora-logs:/app/logs
      - letsencrypt-certs:/etc/letsencrypt:ro
    environment:
      - RUST_LOG=info
      - CONFIG_PATH=/app/config/production.yaml
    depends_on:
      - oauth-service
      - admin-portal
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ============================
  # OAuth Service (Rust)
  # ============================
  oauth-service:
    build:
      context: ./apps/oauth-service-rust
      dockerfile: Dockerfile.production
    container_name: oauth-service-rust
    restart: unless-stopped
    networks:
      oauth-network:
        ipv4_address: 172.20.0.20
    volumes:
      - oauth-db-data:/app/data
      - oauth-logs:/app/logs
      - ./apps/oauth-service-rust/keys:/app/keys:ro  # JWT 密钥 (只读)
      - ./.env.oauth-service:/app/.env:ro            # 环境变量 (只读)
    environment:
      - NODE_ENV=production
      - RUST_LOG=info,oauth_service_rust=debug
      - DATABASE_URL=sqlite:/app/data/oauth.db
      - JWT_ALGORITHM=RS256
      - JWT_PRIVATE_KEY_PATH=/app/keys/private_key.pem
      - JWT_PUBLIC_KEY_PATH=/app/keys/public_key.pem
      - ISSUER=https://auth.yourdomain.com
      - SERVER_HOST=0.0.0.0
      - SERVER_PORT=3001
      - DB_MAX_CONNECTIONS=20
      - SKIP_DB_INIT=false
      - ENABLE_AUDIT_LOG=true
      - LOG_OUTPUT=file
      - LOG_FILE_PATH=/app/logs/oauth-service.log
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "5"
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.5'
          memory: 256M

  # ============================
  # Admin Portal (Next.js)
  # ============================
  admin-portal:
    build:
      context: ./apps/admin-portal
      dockerfile: Dockerfile.production
    container_name: oauth-admin-portal
    restart: unless-stopped
    networks:
      oauth-network:
        ipv4_address: 172.20.0.30
    volumes:
      - admin-portal-data:/app/data
      - ./.env.admin-portal:/app/.env.local:ro
    environment:
      - NODE_ENV=production
      - NEXT_TELEMETRY_DISABLED=1
      - PORT=3002
      - HOSTNAME=0.0.0.0
      - NEXT_PUBLIC_OAUTH_SERVICE_URL=https://api.yourdomain.com/api/v2
      - NEXT_PUBLIC_API_BASE_URL=https://api.yourdomain.com/api/v2
      - NEXT_PUBLIC_OAUTH_CLIENT_ID=admin-portal-client
      - NEXT_PUBLIC_OAUTH_REDIRECT_URI=https://admin.yourdomain.com/auth/callback
      - NEXT_PUBLIC_OAUTH_SCOPE=openid profile email offline_access
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3002/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "5"
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 1024M
        reservations:
          cpus: '1.0'
          memory: 512M
    depends_on:
      - oauth-service

  # ============================
  # MySQL (生产数据库 - 可选)
  # ============================
  mysql:
    image: mysql:8.0
    container_name: oauth-mysql
    restart: unless-stopped
    networks:
      oauth-network:
        ipv4_address: 172.20.0.40
    volumes:
      - ./mysql-data:/var/lib/mysql
      - ./apps/oauth-service-rust/migrations/mysql:/docker-entrypoint-initdb.d:ro
    environment:
      - MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD}
      - MYSQL_DATABASE=oauth_db
      - MYSQL_USER=oauth_user
      - MYSQL_PASSWORD=${MYSQL_PASSWORD}
    command: >
      --character-set-server=utf8mb4
      --collation-server=utf8mb4_unicode_ci
      --default-authentication-plugin=mysql_native_password
      --max-connections=200
      --innodb-buffer-pool-size=1G
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost", "-u", "root", "-p${MYSQL_ROOT_PASSWORD}"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "5"
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2048M
        reservations:
          cpus: '1.0'
          memory: 1024M

  # ============================
  # Redis (分布式缓存 - 可选)
  # ============================
  redis:
    image: redis:7-alpine
    container_name: oauth-redis
    restart: unless-stopped
    networks:
      oauth-network:
        ipv4_address: 172.20.0.50
    volumes:
      - ./redis-data:/data
      - ./redis.conf:/usr/local/etc/redis/redis.conf:ro
    command: redis-server /usr/local/etc/redis/redis.conf
    environment:
      - REDIS_PASSWORD=${REDIS_PASSWORD}
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M

  # ============================
  # Prometheus (监控 - 可选)
  # ============================
  prometheus:
    image: prom/prometheus:latest
    container_name: oauth-prometheus
    restart: unless-stopped
    networks:
      oauth-network:
        ipv4_address: 172.20.0.60
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"

  # ============================
  # Grafana (可视化 - 可选)
  # ============================
  grafana:
    image: grafana/grafana:latest
    container_name: oauth-grafana
    restart: unless-stopped
    networks:
      oauth-network:
        ipv4_address: 172.20.0.70
    ports:
      - "3000:3000"
    volumes:
      - ./grafana-data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD}
      - GF_INSTALL_PLUGINS=grafana-clock-panel
    depends_on:
      - prometheus
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"

  # ============================
  # Loki (日志聚合 - 可选)
  # ============================
  loki:
    image: grafana/loki:latest
    container_name: oauth-loki
    restart: unless-stopped
    networks:
      oauth-network:
        ipv4_address: 172.20.0.80
    ports:
      - "3100:3100"
    volumes:
      - ./loki-data:/loki
      - ./monitoring/loki-config.yml:/etc/loki/local-config.yaml:ro
    command: -config.file=/etc/loki/local-config.yaml
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3100/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"

# ================================
# 注意事项
# ================================
# 1. 创建外部环境变量文件:
#    - .env.oauth-service
#    - .env.admin-portal
#    - .env (用于 MySQL/Redis 密码)
#
# 2. 生成 JWT 密钥对:
#    mkdir -p apps/oauth-service-rust/keys
#    openssl genrsa -out apps/oauth-service-rust/keys/private_key.pem 2048
#    openssl rsa -in apps/oauth-service-rust/keys/private_key.pem -pubout -out apps/oauth-service-rust/keys/public_key.pem
#
# 3. 配置 SSL 证书:
#    - 使用 Let's Encrypt 或其他证书颁发机构
#    - 将证书放置在 letsencrypt-certs volume
#
# 4. 数据备份:
#    - 定期备份 oauth-db-data, mysql-data volumes
#    - 备份命令示例:
#      docker run --rm -v oauth-db-data:/data -v $(pwd):/backup alpine tar czf /backup/oauth-db-backup.tar.gz /data
#
# 5. 安全加固:
#    - 使用 secrets 管理敏感信息
#    - 限制网络访问
#    - 定期更新镜像
#    - 启用容器安全扫描
#
# 6. 性能优化:
#    - 根据实际负载调整资源限制
#    - 配置反向代理缓存
#    - 启用 gzip 压缩
#
# 7. 监控和告警:
#    - 配置 Prometheus 告警规则
#    - 设置 Grafana 仪表板
#    - 集成 Loki 日志查询
